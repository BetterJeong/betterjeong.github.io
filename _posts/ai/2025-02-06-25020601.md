---
title: "[PyTorch] Word2Vec"
excerpt: "CBoW, Skip-gram, 계층적 소프트맥스, 젠심(Gensim)"

categories:
  - AI

toc: false
toc_sticky: false

date: 2025-02-06
last_modified_at: 2025-02-06
---

> AI 스터디를 하며 '파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습' 교재를 정리한 글입니다.  

# Word2Vec

단어 간 유사성 측정을 위해 분포 가설(distributional hypothesis)을 기반으로 개발  
대표적인 단어 임베딩 기법 중 하나  
한국어의 어근, 접사, 조사 등 처럼 단어의 형태학적 특징을 반영하지 못함  

## 분포 가설

단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담음  
같은 문맥에서 자주 나타나는 단어들이 서로 유사한 의미를 가질 가능성이 높음  
단어 간 동시 발생(co-occurrence) 확률 분포로 단어 간 유사성 측정  
유사한 문맥에서 등장하는 단어는 벡터 공간상 비슷한 위치를 가짐  

## 단어 벡터화

희소 표현(sparse representation)과 밀집 표현(dense representation)으로 나눌 수 있음  

### 희소 표현

원-핫 인코딩, TF-IDF와 같이 벡터 요소가 0으로 표현되는 방법  
단어 사전의 크기가 커지면 벡터의 크기도 커져서 공간적 낭비 발생  
단어 간 유사성 반영 불가, 벡터 간 유사성 계산 어려움  

### 밀집 표현

단어를 고정된 크기의 실수 벡터로 표현하므로 단어 사전이 커져도 벡터의 크기가 커지지 않음  
단어 간 거리 계산에 효과적  
벡터가 실수로 구성되어 효율적인 공간 활용 가능  

단어 임베딩 벡터(Word Embedding Vector): 밀집 표현된 벡터를 의미  

## CBoW (Continuous Bag of Words)

주변 단어로 중간 단어를 예측하는 방법  
슬라이딩 윈도로 여러 개의 중심 단어와 주변 단어 학습 가능  

1. 각 입력 단어를 원-핫 벡터로 표현
2. 투사층에 원-핫 벡터 입력
3. 투사층 통과 시 각 단어가 E 크기의 임베딩 벡터로 변환
4. 입력된 단어의 벡터 평균값 계산
5. 계산된 평균 벡터와 가중치 행렬 $W'_{E \times V}$과 곱해 $V$ 크기의 벡터를 얻음
6. 얻은 벡터에 소프트맥스 함수로 중심 단어 예측

중심 단어(Centwr Word): 예측해야 할 단어  
주변 단어(Context Word): 예측에 사용되는 단어  
윈도(Window): 중심 단어를 위해 고려할 주변 단어 수  
슬라이딩 윈도(Sliding Window): window를 이동해가며 학습하는 방법  
투사층(Projection Layer): 원-핫 벡터 인덱스에 해당하는 임베딩 벡터를 반환하는 순람표(Lookup table, LUT) 구조  

## Skip-gram

중심 단어로 주변 단어를 예측하는 모델  
중심 단어와 각 주변 단어를 쌍으로 모델 학습  

1. 입력 단어의 원-핫 벡터를 투사층에 입력  
2. 임베딩 벡터와 $W'_{E \times V}$ 가중치를 곱해 $V$ 크기의 벡터를 얻음  
3. 소프트맥스 연산으로 주변 단어 예측  

### 임베딩 클래스

```python
embedding = torch.nn.Embedding(
    num_embeddings,  # 임베딩 수: 이산 변수 개수 (단어 사전 크기)
    embedding_dim, # 임베딩 차원: 임베딩 벡터 크기
    padding_idx=None, # 패딩 인덱스: 입력 문장 길이를 일정하게 맞추는 역할
    max_norm=None, # 최대 노름: 임베딩 벡터의 최대 크기
    norm_type=2.0 # 노름 타입: 임베딩 벡터 크기 제한하는 방법 선택, 기본값은 2(L2 정규화)
)
```

### CBoW와의 차이점

학습 데이터 구성 방식이 다름  
Skip-gram은 많은 학습 데이터를 추출할 수 있어 일반적으로 CBoW보다 뛰어남  

CBoW: 하나의 윈도에서 하나의 학습 데이터 생성  
Skip-gram: 중심 단어와 주변 단어를 하나의 쌍으로 여러 학습 데이터 생성  

## 계층적 소프트맥스(Hierachical Softmax)

코퍼스가 커지면 Word2Vec 모델 학습 시 학습 속도가 느려지는 단점 완화를 위해 사용  
출력층을 이진 트리 구조로 표현하고 자주 등장하는 단어는 상위 노드, 드물게 등장하는 단어는 하위 노드에 배치  

잎 노드(Leaf Node): 가장 깊은 노드, 각 단어를 의미  
각 단어의 확률은 경로 노드의 확률을 곱해서 구함  

시간 복잡도 비교  
일반적인 소프트맥스 연산: $O(V)$  
계층적 소프트맥스: $O(log_2V)$  

## 네거티브 샘플링(Negative Sampling)

전체 단어 집합에서 일부 단어를 샘플링하여 오답 단어로 사용하는 확률적 샘플링 기법  
학습 윈도 밖의 단어 5~20개를 추출하며, 각 단어가 추출될 확률을 계산할 수 있음  

$$
P(w_i) = \frac{f(w_i)^{0.75}}{\sum_{j=0}^{V} f(w_j)^{0.75}}
$$

$f(w_i)$: 각 단어 $w_i$의 출현 빈도 수(단어 등장 횟수/전체 단어 빈도)  
$P(w_i)$: 단어 $w_i$가 네거티브 샘플로 추출될 확률  
정규화 상수: 출현 빈도수에 0.75제곱  

### 일반 Skip-gram 모델 훈련 데이터와 비교

일반 Skip-gram 모델의 훈련 데이터: 입력 데이터가 중심 단어, 출력 데이터는 윈도  
네거티브 샘플링 모델의 훈련 데이터: 입력 데이터(중심 단어, 윈도 쌍 or 중심 단어, 오답 쌍)를 실제 데이터에서 추출된 쌍일 경우 1, 가짜 단어 쌍일 경우 0으로 이진 분류  

## 젠심(Gensim)

대용량 텍스트 데이터 처리를 위해 메모리 효율적인 방법 제공  
대규모 데이터셋에 효과적  
학습된 모델 저장 관리 가능, 비슷한 단어 찾기 등 유사도 관련된 기능 제공  

### Word2Vec 클래스

```python
word2vec = gensim.models.Word2Vec(
    sentences=None, # 학습 데이터 (토큰 리스트)
    corpus_file=None, # 학습 데이터 파일로 입력 시 경로
    vector_size=100, # 학습할 임베딩 벡터 크기 (차원 수)
    alpha=0.025, # 학습률
    window=5, # 윈도 크기
    min_count=5, # 학습에 사용할 단어의 최소 빈도, 최소 빈도만큼 등장하지 않으면 학습에 사용하지 않음
    workers=3, # 스레드 수
    sg=0, # 1: skip-gram 사용, 0: CBoW 사용
    hs=0, # 1: 계층적 소프트맥스 사용, 0: 사용하지 않음
    cbow_mean=1,
    negative=5, # 네거티브 샘플링 확률 지수 (정규화 상수)
    ns_exponent=0.75,
    max_final_vocab=None, # 단어 사전 최대 크기 (자주 등장한 단어 순)
    epochs=5, # 에포크 수
    batch_words=10000 # 학습 배치 구성 단어 수
)
```

